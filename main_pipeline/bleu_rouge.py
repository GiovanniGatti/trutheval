# -*- coding: utf-8 -*-
"""BLEU-ROUGE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UTskROE-47-kE5vX3_lrhcHEqxhbdocY
"""

!pip install rouge-score
!pip install ragas

import os
import json
import random
import requests
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# For BLEU
import spacy
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

nlp = spacy.load("en_core_web_sm")

# For ROUGE (via rouge-score)
from rouge_score import rouge_scorer
from ragas import evaluate, EvaluationDataset
from ragas.metrics import AnswerCorrectness

# Print the current working directory for debugging purposes
print("Current Working Directory:", os.getcwd())

# Load OpenAI API key from environment variable
openai_key = os.getenv('OPENAI_API_KEY')
if openai_key is None:
    raise ValueError("OpenAI API key not set. Please set the OPENAI_API_KEY environment variable.")

# Load the gold dataset from a JSON file containing questions and answers
with open('/content/sample_data/gold_dataset.json', 'r') as f:
    gold_dataset = json.load(f)

# BLEU Scoring function
def compute_bleu_score_spacy(reference, hypothesis):
    """
    Computes the BLEU score between a reference and a hypothesis sentence using spaCy for tokenization.
    """
    # Tokenization
    reference_tokens = [token.text for token in nlp(reference)]
    hypothesis_tokens = [token.text for token in nlp(hypothesis)]

    # Smoothing to avoid zero points
    smoothing = SmoothingFunction().method1
    bleu = sentence_bleu(
        [reference_tokens],
        hypothesis_tokens,
        smoothing_function=smoothing
    )
    return bleu

def compute_rouge_scores(reference, hypothesis):
    """
    Computes the ROUGE-1, ROUGE-2, and ROUGE-L F1 scores using rouge-score.
    Returns a dict with three ROUGE metrics.
    """
    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)
    scores = scorer.score(reference, hypothesis)

    return {
        'rouge1': scores['rouge1'].fmeasure,
        'rouge2': scores['rouge2'].fmeasure,
        'rougeL': scores['rougeL'].fmeasure
    }


# 2. Existing paraphrase & noise addition logic
def paraphrase_with_openai(answer):
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {openai_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "gpt-4o-mini",
        "messages": [
            {"role": "user", "content": f"Paraphrase the following sentence: '{answer}'"}
        ],
        "max_tokens": 60
    }

    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        paraphrased_answer = response.json()['choices'][0]['message']['content']
        return paraphrased_answer
    else:
        print(f"Error: {response.status_code}, {response.text}")
        return answer  # Fallback to original answer

def add_noise_to_answers(answer, noise_level):
    if noise_level >= 0.1:
        answer = paraphrase_with_openai(answer)
    if noise_level >= 0.5:
        sentences = answer.split('. ')
        random.shuffle(sentences)
        answer = '. '.join(sentences)

        words = answer.split()
        num_words_to_remove = max(1, int(len(words) * 0.1))
        for _ in range(num_words_to_remove):
            if words:
                words.remove(random.choice(words))
        answer = ' '.join(words)

    return answer

noise_levels = [0.1, 0.5, 1.0]
final_dataset = []

questions = gold_dataset['questions']
answers = gold_dataset['answers']

for level in noise_levels:
    for i in range(min(len(questions), len(answers))):
        question = questions[i]['question']
        original_answer = answers[i]['answer']
        noisy_answer = add_noise_to_answers(original_answer, level)
        # Compute BLEU & ROUGE
        bleu_score = compute_bleu_score(original_answer, noisy_answer)
        rouge_dict = compute_rouge_scores(original_answer, noisy_answer)

        final_dataset.append({
            'user_input': question,
            'reference': original_answer,
            'response': noisy_answer,
            'noise_level': level,
            'bleu_score': bleu_score,
            'rouge1': rouge_dict['rouge1'],
            'rouge2': rouge_dict['rouge2'],
            'rougeL': rouge_dict['rougeL']
        })

print("Final Dataset with Noise:")
for item in final_dataset:
    print(item)

# 3. Evaluate with Ragas

def ragas_evaluation(final_data):
    metrics = [AnswerCorrectness()]
    dataset = EvaluationDataset.from_dict(final_data)
    results = evaluate(dataset=dataset, metrics=metrics)
    return results

ragas_results = ragas_evaluation(final_dataset)
df = ragas_results.to_pandas()
df['noise_level'] = [item['noise_level'] for item in final_dataset]

print("\nRagas Evaluation Results for Final Dataset:")
print(df)

# Convert final_dataset to a DataFrame for saving
final_df = pd.DataFrame(final_dataset)
final_df.to_csv('final_dataset_with_noise.csv', index=False)

# Save the Ragas results
ragas_df = df[['user_input', 'response', 'reference', 'answer_correctness', 'noise_level']]
ragas_df.to_csv('ragas_evaluation_results.csv', index=False)

# Example of manual evaluations with numerical scores
manual_evaluations = {0.1: 8, 0.5: 6, 1.0: 3}

def compare_evaluations(manual_scores, ragas_results):
    evaluation_results = {}
    ragas_scores = ragas_results.groupby('noise_level')['answer_correctness'].mean().to_dict()
    for level in manual_scores.keys():
        manual_score = manual_scores[level]
        ragas_score = ragas_scores.get(level, None)
        if ragas_score is None:
            print(f"Warning: No Ragas score found for noise level {level}")
            continue
        difference = abs(manual_score - ragas_score)
        evaluation_quality = (
            "Poorly Evaluated" if difference > 5
            else "Moderately Evaluated" if difference > 2
            else "Well Evaluated"
        )
        evaluation_results[level] = {
            'manual_score': manual_score,
            'ragas_score': ragas_score,
            'difference': difference,
            'evaluation_quality': evaluation_quality
        }
    return evaluation_results

evaluation_results = compare_evaluations(manual_evaluations, df)

levels = list(evaluation_results.keys())
manual_scores = [evaluation_results[level]['manual_score'] for level in levels]
ragas_scores = [evaluation_results[level]['ragas_score'] for level in levels]
differences = [evaluation_results[level]['difference'] for level in levels]
evaluation_qualities = [evaluation_results[level]['evaluation_quality'] for level in levels]

plt.figure(figsize=(10, 6))
bar_width = 0.2
index = range(len(levels))

plt.bar(index, manual_scores, bar_width, label='Manual Scores', color='b', alpha=0.6)
plt.bar([i + bar_width for i in index], ragas_scores, bar_width, label='Ragas Scores', color='g', alpha=0.6)
plt.bar([i + 2 * bar_width for i in index], differences, bar_width, label='Difference', color='r', alpha=0.6)

plt.xlabel('Noise Level')
plt.ylabel('Scores')
plt.title('Evaluation Comparison Results')
plt.xticks([i + bar_width for i in index], levels)
plt.legend()

for i, level in enumerate(levels):
    plt.text(i, manual_scores[i] + 0.5, evaluation_qualities[i], ha='center', va='bottom')

plt.tight_layout()
plt.show()